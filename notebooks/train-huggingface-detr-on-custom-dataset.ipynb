{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ea3rMbjMN8kl"
      },
      "source": [
        "[![Roboflow Notebooks](https://media.roboflow.com/notebooks/template/bannertest2-2.png?ik-sdk-version=javascript-1.4.3&updatedAt=1672932710194)](https://github.com/roboflow/notebooks)\n",
        "\n",
        "# How to Train DETR with ü§ó Transformers on a Custom Dataset\n",
        "\n",
        "---\n",
        "\n",
        "[![Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/roboflow-ai/notebooks/blob/main/notebooks/train-huggingface-detr-on-custom-dataset.ipynb) [![GitHub](https://badges.aleen42.com/src/github.svg)](https://github.com/roboflow/notebooks/blob/main/notebooks/train-huggingface-detr-on-custom-dataset.ipynb) [![arXiv](https://img.shields.io/badge/arXiv-2005.12872-b31b1b.svg)](https://arxiv.org/abs/2005.12872) [![YouTube](https://badges.aleen42.com/src/youtube.svg)](https://youtu.be/AM8D4j9KoaU) [![Roboflow](https://app.roboflow.com/images/download-dataset-badge.svg)](https://universe.roboflow.com/roboflow-jvuqo/football-players-detection-3zvbc)\n",
        "\n",
        "This notebook is based on [Niels Rogge](https://github.com/NielsRogge) Fine-tuning DETR on a custom dataset for object detection [notebook](https://github.com/NielsRogge/Transformers-Tutorials/blob/master/DETR/Fine_tuning_DetrForObjectDetection_on_custom_dataset_(balloon).ipynb). In this notebook, we are going to fine-tune [DETR](https://ai.facebook.com/blog/end-to-end-object-detection-with-transformers/) (end-to-end object detection with Transformers) on a custom object detection dataset. The goal for the model is to detect players on football field.\n",
        "\n",
        "- Original DETR paper: https://arxiv.org/abs/2005.12872\n",
        "- Original DETR repo: https://github.com/facebookresearch/detr\n",
        "\n",
        "DETR paper - [End-to-End Object Detection with Transformers](https://arxiv.org/abs/2005.12872) was released in 2020 and since then there were already several follow-ups:\n",
        "- [Deformable DETR](https://huggingface.co/docs/transformers/main/en/model_doc/deformable_detr)\n",
        "- [Conditional DETR](https://huggingface.co/docs/transformers/main/en/model_doc/conditional_detr)\n",
        "- [YOLOS](https://blog.roboflow.com/train-yolos-transformer-custom-dataset)\n",
        "- [DETA](https://huggingface.co/docs/transformers/main/en/model_doc/deta)\n",
        "\n",
        "They all have the same API in ü§ó Transformers library, so training them is mostly equivalent.\n",
        "\n",
        "## Pro Tip: Use GPU Acceleration\n",
        "\n",
        "If you are running this notebook in Google Colab, navigate to `Edit` -> `Notebook settings` -> `Hardware accelerator`, set it to `GPU`, and then click `Save`. This will ensure your notebook uses a GPU, which will significantly speed up model training times.\n",
        "\n",
        "## Steps in this Tutorial\n",
        "\n",
        "In this tutorial, we are going to cover:\n",
        "\n",
        "- Before you start\n",
        "- Environment setup\n",
        "- Inference with pre-trained COCO model\n",
        "- Roboflow Universe\n",
        "- Preparing a custom dataset\n",
        "- Download custom dataset\n",
        "- Create COCO data loaders\n",
        "- Visualize data entry\n",
        "- Train model with PyTorch Lightning\n",
        "- Inference on test dataset\n",
        "- Evaluation on test dataset\n",
        "- Save and load model\n",
        "\n",
        "**Let's begin!**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nvcVg3e2eEH9"
      },
      "source": [
        "## Before you start\n",
        "\n",
        "Let's make sure that we have access to GPU. We can use `nvidia-smi` command to do that. In case of any problems navigate to `Edit` -> `Notebook settings` -> `Hardware accelerator`, set it to `GPU`, and then click `Save`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ihwd6wbQeFdk"
      },
      "outputs": [],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "idyl7Hltwev8"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "HOME = os.getcwd()\n",
        "print(HOME)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fQ1xdhF9epQ5"
      },
      "source": [
        "## Environment setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1s3GNNh3yKkL"
      },
      "outputs": [],
      "source": [
        "!pip install -i https://test.pypi.org/simple/ supervision==0.3.0\n",
        "!pip install -q transformers\n",
        "!pip install -q pytorch-lightning\n",
        "!pip install -q roboflow\n",
        "!pip install -q timm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hkBxWG_koPsg"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "!nvcc --version\n",
        "TORCH_VERSION = \".\".join(torch.__version__.split(\".\")[:2])\n",
        "CUDA_VERSION = torch.__version__.split(\"+\")[-1]\n",
        "print(\"torch: \", TORCH_VERSION, \"; cuda: \", CUDA_VERSION)\n",
        "\n",
        "import roboflow\n",
        "import supervision\n",
        "import transformers\n",
        "import pytorch_lightning\n",
        "\n",
        "print(\n",
        "    \"roboflow:\", roboflow.__version__,\n",
        "    \"; supervision:\", supervision.__version__,\n",
        "    \"; transformers:\", transformers.__version__,\n",
        "    \"; pytorch_lightning:\", pytorch_lightning.__version__\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fe6r7u1bffJ1"
      },
      "source": [
        "## Inference with pre-trained COCO model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hddhx7oPgHXJ"
      },
      "source": [
        "### Download Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_ij8_XssfkQg"
      },
      "outputs": [],
      "source": [
        "%cd {HOME}\n",
        "!wget https://media.roboflow.com/notebooks/examples/dog.jpeg"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V4gxmpNJgRbH"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "IMAGE_NAME = \"dog.jpeg\"\n",
        "IMAGE_PATH = os.path.join(HOME, IMAGE_NAME)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wHQTqHsQgsCZ"
      },
      "source": [
        "### Load Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w70Rgx-ydq2L"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import DetrForObjectDetection, DetrImageProcessor\n",
        "\n",
        "\n",
        "# settings\n",
        "DEVICE = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "CHECKPOINT = 'facebook/detr-resnet-50'\n",
        "CONFIDENCE_TRESHOLD = 0.5\n",
        "IOU_TRESHOLD = 0.8\n",
        "\n",
        "image_processor = DetrImageProcessor.from_pretrained(CHECKPOINT)\n",
        "model = DetrForObjectDetection.from_pretrained(CHECKPOINT)\n",
        "model.to(DEVICE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z95T5a3ciF6b"
      },
      "source": [
        "### Inference"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ToSzLwjHiMP_"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "import torch\n",
        "import supervision as sv\n",
        "\n",
        "\n",
        "with torch.no_grad():\n",
        "\n",
        "    # load image and predict\n",
        "    image = cv2.imread(IMAGE_PATH)\n",
        "    inputs = image_processor(images=image, return_tensors='pt').to(DEVICE)\n",
        "    outputs = model(**inputs)\n",
        "\n",
        "    # post-process\n",
        "    target_sizes = torch.tensor([image.shape[:2]]).to(DEVICE)\n",
        "    results = image_processor.post_process_object_detection(\n",
        "        outputs=outputs,\n",
        "        threshold=CONFIDENCE_TRESHOLD,\n",
        "        target_sizes=target_sizes\n",
        "    )[0]\n",
        "\n",
        "# annotate\n",
        "detections = sv.Detections.from_transformers(transformers_results=results)\n",
        "\n",
        "labels = [\n",
        "    f\"{model.config.id2label[class_id]} {confidence:0.2f}\"\n",
        "    for _, confidence, class_id, _\n",
        "    in detections\n",
        "]\n",
        "\n",
        "box_annotator = sv.BoxAnnotator()\n",
        "frame = box_annotator.annotate(scene=image, detections=detections, labels=labels)\n",
        "\n",
        "%matplotlib inline\n",
        "sv.show_frame_in_notebook(frame, (16, 16))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5nrM8p9QuhZC"
      },
      "source": [
        "**NOTE:** It seems that we have a lot of excess detections. Let's try to filter them out using non-max suppression."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ehle9xKWzhTP"
      },
      "source": [
        "## Inference + NMS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vRD7HQCZzRUz"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "import torch\n",
        "import supervision as sv\n",
        "\n",
        "\n",
        "with torch.no_grad():\n",
        "\n",
        "    # load image and predict\n",
        "    image = cv2.imread(IMAGE_PATH)\n",
        "    inputs = image_processor(images=image, return_tensors='pt').to(DEVICE)\n",
        "    outputs = model(**inputs)\n",
        "\n",
        "    # post-process\n",
        "    target_sizes = torch.tensor([image.shape[:2]]).to(DEVICE)\n",
        "    results = image_processor.post_process_object_detection(\n",
        "        outputs=outputs,\n",
        "        threshold=CONFIDENCE_TRESHOLD,\n",
        "        target_sizes=target_sizes\n",
        "    )[0]\n",
        "\n",
        "# annotate\n",
        "detections = sv.Detections.from_transformers(transformers_results=results).with_nms(threshold=IOU_TRESHOLD)\n",
        "\n",
        "labels = [\n",
        "    f\"{model.config.id2label[class_id]} {confidence:0.2f}\"\n",
        "    for _, confidence, class_id, _\n",
        "    in detections\n",
        "]\n",
        "\n",
        "box_annotator = sv.BoxAnnotator()\n",
        "frame = box_annotator.annotate(scene=image, detections=detections, labels=labels)\n",
        "\n",
        "%matplotlib inline\n",
        "sv.show_frame_in_notebook(frame, (16, 16))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MPTtTKLSvyoC"
      },
      "source": [
        "## Roboflow Universe\n",
        "\n",
        "Need data for your project? Before spending time on annotating, check out Roboflow Universe, a repository of more than 110,000 open-source datasets that you can use in your projects. You'll find datasets containing everything from annotated cracks in concrete to plant images with disease annotations.\n",
        "\n",
        "\n",
        "[![Roboflow Universe](https://media.roboflow.com/notebooks/template/uni-banner-frame.png?ik-sdk-version=javascript-1.4.3&updatedAt=1672878480290)](https://universe.roboflow.com/)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "shYmn4Qjv5ao"
      },
      "source": [
        "## Preparing a custom¬†dataset\n",
        "\n",
        "Building a custom dataset can be a painful process. It might take dozens or even hundreds of hours to collect images, label them, and export them in the proper format. Fortunately, Roboflow makes this process as straightforward and fast as possible. Let me show you how!\n",
        "\n",
        "### Step 1: Creating project\n",
        "\n",
        "Before you start, you need to create a Roboflow [account](https://app.roboflow.com/login). Once you do that, you can create a new project in the Roboflow [dashboard](https://app.roboflow.com/). Keep in mind to choose the right project type. In our case, Object Detection.\n",
        "\n",
        "<div align=\"center\">\n",
        "  <img\n",
        "    width=\"640\"\n",
        "    src=\"https://media.roboflow.com/preparing-custom-dataset-example/creating-project.gif?ik-sdk-version=javascript-1.4.3&updatedAt=1672929799852\"\n",
        "  >\n",
        "</div>\n",
        "\n",
        "### Step 2: Uploading images\n",
        "\n",
        "Next, add the data to your newly created project. You can do it via API or through our [web interface](https://docs.roboflow.com/adding-data/object-detection).\n",
        "\n",
        "If you drag and drop a directory with a dataset in a supported format, the Roboflow dashboard will automatically read the images and annotations together.\n",
        "\n",
        "<div align=\"center\">\n",
        "  <img\n",
        "    width=\"640\"\n",
        "    src=\"https://media.roboflow.com/preparing-custom-dataset-example/uploading-images.gif?ik-sdk-version=javascript-1.4.3&updatedAt=1672929808290\"\n",
        "  >\n",
        "</div>\n",
        "\n",
        "### Step 3: Labeling\n",
        "\n",
        "If you only have images, you can label them in [Roboflow Annotate](https://docs.roboflow.com/annotate).\n",
        "\n",
        "<div align=\"center\">\n",
        "  <img\n",
        "    width=\"640\"\n",
        "    src=\"https://user-images.githubusercontent.com/26109316/210901980-04861efd-dfc0-4a01-9373-13a36b5e1df4.gif\"\n",
        "  >\n",
        "</div>\n",
        "\n",
        "### Step 4: Generate new dataset version\n",
        "\n",
        "Now that we have our images and annotations added, we can Generate a Dataset Version. When Generating a Version, you may elect to add preprocessing and augmentations. This step is completely optional, however, it can allow you to significantly improve the robustness of your model.\n",
        "\n",
        "<div align=\"center\">\n",
        "  <img\n",
        "    width=\"640\"\n",
        "    src=\"https://media.roboflow.com/preparing-custom-dataset-example/generate-new-version.gif?ik-sdk-version=javascript-1.4.3&updatedAt=1673003597834\"\n",
        "  >\n",
        "</div>\n",
        "\n",
        "### Step 5: Exporting dataset\n",
        "\n",
        "Once the dataset version is generated, we have a hosted dataset we can load directly into our notebook for easy training. Click `Export` and select the `YOLO v5 PyTorch` dataset format.\n",
        "\n",
        "<div align=\"center\">\n",
        "  <img\n",
        "    width=\"640\"\n",
        "    src=\"https://media.roboflow.com/preparing-custom-dataset-example/export.gif?ik-sdk-version=javascript-1.4.3&updatedAt=1672943313709\"\n",
        "  >\n",
        "</div>\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ut0S6jbzfYFW"
      },
      "source": [
        "## Download custom dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CHYhr6xKx_O_"
      },
      "source": [
        "[![Roboflow](https://app.roboflow.com/images/download-dataset-badge.svg)](https://universe.roboflow.com/roboflow-jvuqo/football-players-detection-3zvbc)\n",
        "\n",
        "**NOTE:** Note that we download the dataset in `coco` format."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from getpass import getpass\n",
        "\n",
        "ROBOFLOW_API_KEY = getpass('Enter ROBOFLOW_API_KEY secret value: ')"
      ],
      "metadata": {
        "id": "1hkRhqPEC1tL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H_APffCi2eY9"
      },
      "outputs": [],
      "source": [
        "!mkdir {HOME}/datasets\n",
        "%cd {HOME}/datasets\n",
        "\n",
        "from roboflow import Roboflow\n",
        "rf = Roboflow(api_key=ROBOFLOW_API_KEY)\n",
        "project = rf.workspace(\"roboflow-jvuqo\").project(\"football-players-detection-3zvbc\")\n",
        "dataset = project.version(1).download(\"coco\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DXKyMYI4vTv9"
      },
      "source": [
        "**NOTE:** We can find out where our dataset was saved using the `dataset.location` property."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pH8suSKrmz5_"
      },
      "outputs": [],
      "source": [
        "dataset.location"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K0gIu1AJrAeg"
      },
      "source": [
        "## Create COCO data loaders"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qk6sRB0lueHY"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import torchvision\n",
        "\n",
        "\n",
        "# settings\n",
        "ANNOTATION_FILE_NAME = \"_annotations.coco.json\"\n",
        "TRAIN_DIRECTORY = os.path.join(dataset.location, \"train\")\n",
        "VAL_DIRECTORY = os.path.join(dataset.location, \"valid\")\n",
        "TEST_DIRECTORY = os.path.join(dataset.location, \"test\")\n",
        "\n",
        "\n",
        "class CocoDetection(torchvision.datasets.CocoDetection):\n",
        "    def __init__(\n",
        "        self,\n",
        "        image_directory_path: str,\n",
        "        image_processor,\n",
        "        train: bool = True\n",
        "    ):\n",
        "        annotation_file_path = os.path.join(image_directory_path, ANNOTATION_FILE_NAME)\n",
        "        super(CocoDetection, self).__init__(image_directory_path, annotation_file_path)\n",
        "        self.image_processor = image_processor\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        images, annotations = super(CocoDetection, self).__getitem__(idx)\n",
        "        image_id = self.ids[idx]\n",
        "        annotations = {'image_id': image_id, 'annotations': annotations}\n",
        "        encoding = self.image_processor(images=images, annotations=annotations, return_tensors=\"pt\")\n",
        "        pixel_values = encoding[\"pixel_values\"].squeeze()\n",
        "        target = encoding[\"labels\"][0]\n",
        "\n",
        "        return pixel_values, target\n",
        "\n",
        "\n",
        "TRAIN_DATASET = CocoDetection(\n",
        "    image_directory_path=TRAIN_DIRECTORY,\n",
        "    image_processor=image_processor,\n",
        "    train=True)\n",
        "VAL_DATASET = CocoDetection(\n",
        "    image_directory_path=VAL_DIRECTORY,\n",
        "    image_processor=image_processor,\n",
        "    train=False)\n",
        "TEST_DATASET = CocoDetection(\n",
        "    image_directory_path=TEST_DIRECTORY,\n",
        "    image_processor=image_processor,\n",
        "    train=False)\n",
        "\n",
        "print(\"Number of training examples:\", len(TRAIN_DATASET))\n",
        "print(\"Number of validation examples:\", len(VAL_DATASET))\n",
        "print(\"Number of test examples:\", len(TEST_DATASET))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OeT9rfsMwj5O"
      },
      "source": [
        "## Visualize data entry"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "85pfhh5FwoVm"
      },
      "source": [
        "**NOTE:** Feel free to reload this cell multiple times. Notebook should display different train set image each time."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZoRf4rTewQKc"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "import cv2\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "# select random image\n",
        "image_ids = TRAIN_DATASET.coco.getImgIds()\n",
        "image_id = random.choice(image_ids)\n",
        "print('Image #{}'.format(image_id))\n",
        "\n",
        "# load image and annotatons\n",
        "image = TRAIN_DATASET.coco.loadImgs(image_id)[0]\n",
        "annotations = TRAIN_DATASET.coco.imgToAnns[image_id]\n",
        "image_path = os.path.join(TRAIN_DATASET.root, image['file_name'])\n",
        "image = cv2.imread(image_path)\n",
        "\n",
        "# annotate\n",
        "detections = sv.Detections.from_coco_annotations(coco_annotation=annotations)\n",
        "\n",
        "# we will use id2label function for training\n",
        "categories = TRAIN_DATASET.coco.cats\n",
        "id2label = {k: v['name'] for k,v in categories.items()}\n",
        "\n",
        "labels = [\n",
        "    f\"{id2label[class_id]}\"\n",
        "    for _, _, class_id, _\n",
        "    in detections\n",
        "]\n",
        "\n",
        "box_annotator = sv.BoxAnnotator()\n",
        "frame = box_annotator.annotate(scene=image, detections=detections, labels=labels)\n",
        "\n",
        "%matplotlib inline\n",
        "sv.show_frame_in_notebook(image, (16, 16))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3OPAnqJ-wxod"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "def collate_fn(batch):\n",
        "    # DETR authors employ various image sizes during training, making it not possible\n",
        "    # to directly batch together images. Hence they pad the images to the biggest\n",
        "    # resolution in a given batch, and create a corresponding binary pixel_mask\n",
        "    # which indicates which pixels are real/which are padding\n",
        "    pixel_values = [item[0] for item in batch]\n",
        "    encoding = image_processor.pad(pixel_values, return_tensors=\"pt\")\n",
        "    labels = [item[1] for item in batch]\n",
        "    return {\n",
        "        'pixel_values': encoding['pixel_values'],\n",
        "        'pixel_mask': encoding['pixel_mask'],\n",
        "        'labels': labels\n",
        "    }\n",
        "\n",
        "TRAIN_DATALOADER = DataLoader(dataset=TRAIN_DATASET, collate_fn=collate_fn, batch_size=4, shuffle=True)\n",
        "VAL_DATALOADER = DataLoader(dataset=VAL_DATASET, collate_fn=collate_fn, batch_size=4)\n",
        "TEST_DATALOADER = DataLoader(dataset=TEST_DATASET, collate_fn=collate_fn, batch_size=4)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4vVPm-sO4C9W"
      },
      "source": [
        "## Train model with PyTorch Lightning\n",
        "\n",
        "**NOTE:** Here we define a regular PyTorch dataset. Each item of the dataset is an image and corresponding annotations. Torchvision already provides a `CocoDetection` dataset, which we can use. We only add a feature extractor (`DetrImageProcessor`) to resize + normalize the images, and to turn the annotations (which are in COCO format) in the format that DETR expects. It will also resize the annotations accordingly."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pzOJhEPa39dZ"
      },
      "outputs": [],
      "source": [
        "import pytorch_lightning as pl\n",
        "from transformers import DetrForObjectDetection\n",
        "import torch\n",
        "\n",
        "\n",
        "class Detr(pl.LightningModule):\n",
        "\n",
        "    def __init__(self, lr, lr_backbone, weight_decay):\n",
        "        super().__init__()\n",
        "        self.model = DetrForObjectDetection.from_pretrained(\n",
        "            pretrained_model_name_or_path=CHECKPOINT,\n",
        "            num_labels=len(id2label),\n",
        "            ignore_mismatched_sizes=True\n",
        "        )\n",
        "\n",
        "        self.lr = lr\n",
        "        self.lr_backbone = lr_backbone\n",
        "        self.weight_decay = weight_decay\n",
        "\n",
        "    def forward(self, pixel_values, pixel_mask):\n",
        "        return self.model(pixel_values=pixel_values, pixel_mask=pixel_mask)\n",
        "\n",
        "    def common_step(self, batch, batch_idx):\n",
        "        pixel_values = batch[\"pixel_values\"]\n",
        "        pixel_mask = batch[\"pixel_mask\"]\n",
        "        labels = [{k: v.to(self.device) for k, v in t.items()} for t in batch[\"labels\"]]\n",
        "\n",
        "        outputs = self.model(pixel_values=pixel_values, pixel_mask=pixel_mask, labels=labels)\n",
        "\n",
        "        loss = outputs.loss\n",
        "        loss_dict = outputs.loss_dict\n",
        "\n",
        "        return loss, loss_dict\n",
        "\n",
        "    def training_step(self, batch, batch_idx):\n",
        "        loss, loss_dict = self.common_step(batch, batch_idx)\n",
        "        # logs metrics for each training_step, and the average across the epoch\n",
        "        self.log(\"training_loss\", loss)\n",
        "        for k,v in loss_dict.items():\n",
        "            self.log(\"train_\" + k, v.item())\n",
        "\n",
        "        return loss\n",
        "\n",
        "    def validation_step(self, batch, batch_idx):\n",
        "        loss, loss_dict = self.common_step(batch, batch_idx)\n",
        "        self.log(\"validation/loss\", loss)\n",
        "        for k, v in loss_dict.items():\n",
        "            self.log(\"validation_\" + k, v.item())\n",
        "\n",
        "        return loss\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        # DETR authors decided to use different learning rate for backbone\n",
        "        # you can learn more about it here:\n",
        "        # - https://github.com/facebookresearch/detr/blob/3af9fa878e73b6894ce3596450a8d9b89d918ca9/main.py#L22-L23\n",
        "        # - https://github.com/facebookresearch/detr/blob/3af9fa878e73b6894ce3596450a8d9b89d918ca9/main.py#L131-L139\n",
        "        param_dicts = [\n",
        "            {\n",
        "                \"params\": [p for n, p in self.named_parameters() if \"backbone\" not in n and p.requires_grad]},\n",
        "            {\n",
        "                \"params\": [p for n, p in self.named_parameters() if \"backbone\" in n and p.requires_grad],\n",
        "                \"lr\": self.lr_backbone,\n",
        "            },\n",
        "        ]\n",
        "        return torch.optim.AdamW(param_dicts, lr=self.lr, weight_decay=self.weight_decay)\n",
        "\n",
        "    def train_dataloader(self):\n",
        "        return TRAIN_DATALOADER\n",
        "\n",
        "    def val_dataloader(self):\n",
        "        return VAL_DATALOADER"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ke7XS8_C1UUM"
      },
      "source": [
        "**NOTE:** Let's start `tensorboard`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UGKC5yOyHAIJ"
      },
      "outputs": [],
      "source": [
        "%cd {HOME}\n",
        "\n",
        "%load_ext tensorboard\n",
        "%tensorboard --logdir lightning_logs/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GXZCQ6j-F9kr"
      },
      "outputs": [],
      "source": [
        "model = Detr(lr=1e-4, lr_backbone=1e-5, weight_decay=1e-4)\n",
        "\n",
        "batch = next(iter(TRAIN_DATALOADER))\n",
        "outputs = model(pixel_values=batch['pixel_values'], pixel_mask=batch['pixel_mask'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RSPRLP6aGOpV"
      },
      "outputs": [],
      "source": [
        "outputs.logits.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w-cJEJvAGjTB"
      },
      "outputs": [],
      "source": [
        "from pytorch_lightning import Trainer\n",
        "\n",
        "%cd {HOME}\n",
        "\n",
        "# settings\n",
        "MAX_EPOCHS = 50\n",
        "\n",
        "# pytorch_lightning < 2.0.0\n",
        "# trainer = Trainer(gpus=1, max_epochs=MAX_EPOCHS, gradient_clip_val=0.1, accumulate_grad_batches=8, log_every_n_steps=5)\n",
        "\n",
        "# pytorch_lightning >= 2.0.0\n",
        "trainer = Trainer(devices=1, accelerator=\"gpu\", max_epochs=MAX_EPOCHS, gradient_clip_val=0.1, accumulate_grad_batches=8, log_every_n_steps=5)\n",
        "\n",
        "trainer.fit(model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XrewgWdGQULA"
      },
      "source": [
        "## Inference on test dataset\n",
        "\n",
        "Let's visualize the predictions of DETR on the first image of the validation set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A6YfGgeATvbR"
      },
      "outputs": [],
      "source": [
        "model.to(DEVICE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jbzTzHJW22up"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "import cv2\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "# utils\n",
        "categories = TEST_DATASET.coco.cats\n",
        "id2label = {k: v['name'] for k,v in categories.items()}\n",
        "box_annotator = sv.BoxAnnotator()\n",
        "\n",
        "# select random image\n",
        "image_ids = TEST_DATASET.coco.getImgIds()\n",
        "image_id = random.choice(image_ids)\n",
        "print('Image #{}'.format(image_id))\n",
        "\n",
        "# load image and annotatons\n",
        "image = TEST_DATASET.coco.loadImgs(image_id)[0]\n",
        "annotations = TEST_DATASET.coco.imgToAnns[image_id]\n",
        "image_path = os.path.join(TEST_DATASET.root, image['file_name'])\n",
        "image = cv2.imread(image_path)\n",
        "\n",
        "# annotate\n",
        "detections = sv.Detections.from_coco_annotations(coco_annotation=annotations)\n",
        "labels = [f\"{id2label[class_id]}\" for _, _, class_id, _ in detections]\n",
        "frame = box_annotator.annotate(scene=image.copy(), detections=detections, labels=labels)\n",
        "\n",
        "print('ground truth')\n",
        "%matplotlib inline\n",
        "sv.show_frame_in_notebook(frame, (16, 16))\n",
        "\n",
        "# inference\n",
        "with torch.no_grad():\n",
        "\n",
        "    # load image and predict\n",
        "    inputs = image_processor(images=image, return_tensors='pt').to(DEVICE)\n",
        "    outputs = model(**inputs)\n",
        "\n",
        "    # post-process\n",
        "    target_sizes = torch.tensor([image.shape[:2]]).to(DEVICE)\n",
        "    results = image_processor.post_process_object_detection(\n",
        "        outputs=outputs,\n",
        "        threshold=CONFIDENCE_TRESHOLD,\n",
        "        target_sizes=target_sizes\n",
        "    )[0]\n",
        "\n",
        "# annotate\n",
        "detections = sv.Detections.from_transformers(transformers_results=results).with_nms(threshold=0.5)\n",
        "labels = [f\"{id2label[class_id]} {confidence:.2f}\" for _, confidence, class_id, _ in detections]\n",
        "frame = box_annotator.annotate(scene=image.copy(), detections=detections, labels=labels)\n",
        "\n",
        "print('detections')\n",
        "%matplotlib inline\n",
        "sv.show_frame_in_notebook(frame, (16, 16))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y695PeIASOjF"
      },
      "source": [
        "## Evaluation on test dataset\n",
        "\n",
        "Finally, we evaluate the model on the `TEST_DATASET`. For this we make use of the `CocoEvaluator` class available in a tiny [PyPi package](https://github.com/NielsRogge/coco-eval) made by [Niels Rogge](https://github.com/NielsRogge) . This class is entirely based on the [original evaluator](https://github.com/facebookresearch/detr/blob/main/datasets/coco_eval.py) class used by the DETR authors."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q coco_eval"
      ],
      "metadata": {
        "id": "fiEaI_vpSz2i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def convert_to_xywh(boxes):\n",
        "    xmin, ymin, xmax, ymax = boxes.unbind(1)\n",
        "    return torch.stack((xmin, ymin, xmax - xmin, ymax - ymin), dim=1)\n",
        "\n",
        "def prepare_for_coco_detection(predictions):\n",
        "    coco_results = []\n",
        "    for original_id, prediction in predictions.items():\n",
        "        if len(prediction) == 0:\n",
        "            continue\n",
        "\n",
        "        boxes = prediction[\"boxes\"]\n",
        "        boxes = convert_to_xywh(boxes).tolist()\n",
        "        scores = prediction[\"scores\"].tolist()\n",
        "        labels = prediction[\"labels\"].tolist()\n",
        "\n",
        "        coco_results.extend(\n",
        "            [\n",
        "                {\n",
        "                    \"image_id\": original_id,\n",
        "                    \"category_id\": labels[k],\n",
        "                    \"bbox\": box,\n",
        "                    \"score\": scores[k],\n",
        "                }\n",
        "                for k, box in enumerate(boxes)\n",
        "            ]\n",
        "        )\n",
        "    return coco_results"
      ],
      "metadata": {
        "id": "pQx9pgaHVScS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from coco_eval import CocoEvaluator\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "evaluator = CocoEvaluator(coco_gt=TEST_DATASET.coco, iou_types=[\"bbox\"])\n",
        "\n",
        "print(\"Running evaluation...\")\n",
        "\n",
        "for idx, batch in enumerate(tqdm(TEST_DATALOADER)):\n",
        "    pixel_values = batch[\"pixel_values\"].to(DEVICE)\n",
        "    pixel_mask = batch[\"pixel_mask\"].to(DEVICE)\n",
        "    labels = [{k: v.to(DEVICE) for k, v in t.items()} for t in batch[\"labels\"]]\n",
        "\n",
        "    with torch.no_grad():\n",
        "      outputs = model(pixel_values=pixel_values, pixel_mask=pixel_mask)\n",
        "\n",
        "    orig_target_sizes = torch.stack([target[\"orig_size\"] for target in labels], dim=0)\n",
        "    results = image_processor.post_process_object_detection(outputs, target_sizes=orig_target_sizes)\n",
        "\n",
        "    predictions = {target['image_id'].item(): output for target, output in zip(labels, results)}\n",
        "    predictions = prepare_for_coco_detection(predictions)\n",
        "    evaluator.update(predictions)\n",
        "\n",
        "evaluator.synchronize_between_processes()\n",
        "evaluator.accumulate()\n",
        "evaluator.summarize()"
      ],
      "metadata": {
        "id": "Ts6bI87iTZzY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Save and load model"
      ],
      "metadata": {
        "id": "dnDuaKqGW6Bl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "MODEL_PATH = os.path.join(HOME, 'custom-model')"
      ],
      "metadata": {
        "id": "-FPFeXx2XgcW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.model.save_pretrained(MODEL_PATH)"
      ],
      "metadata": {
        "id": "VYIAdWm9W3gY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = DetrForObjectDetection.from_pretrained(MODEL_PATH)\n",
        "model.to(DEVICE)"
      ],
      "metadata": {
        "id": "tHTcJ9UHYfZn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TtB6wKf0C146"
      },
      "source": [
        "## üèÜ Congratulations\n",
        "\n",
        "### Learning Resources\n",
        "\n",
        "Roboflow has produced many resources that you may find interesting as you advance your knowledge of computer vision:\n",
        "\n",
        "- [Roboflow Notebooks](https://github.com/roboflow/notebooks): A repository of over 20 notebooks that walk through how to train custom models with a range of model types, from YOLOv7 to SegFormer.\n",
        "- [Roboflow YouTube](https://www.youtube.com/c/Roboflow): Our library of videos featuring deep dives into the latest in computer vision, detailed tutorials that accompany our notebooks, and more.\n",
        "- [Roboflow Discuss](https://discuss.roboflow.com/): Have a question about how to do something on Roboflow? Ask your question on our discussion forum.\n",
        "- [Roboflow Models](https://roboflow.com): Learn about state-of-the-art models and their performance. Find links and tutorials to guide your learning.\n",
        "\n",
        "### Convert data formats\n",
        "\n",
        "Roboflow provides free utilities to convert data between dozens of popular computer vision formats. Check out [Roboflow Formats](https://roboflow.com/formats) to find tutorials on how to convert data between formats in a few clicks.\n",
        "\n",
        "### Connect computer vision to your project logic\n",
        "\n",
        "[Roboflow Templates](https://roboflow.com/templates) is a public gallery of code snippets that you can use to connect computer vision to your project logic. Code snippets range from sending emails after inference to measuring object distance between detections...\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}